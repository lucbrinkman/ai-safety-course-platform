{
  "video_id": "4l7Is6vOAOA",
  "title": "General AI Won't Want You To Fix its Code - Computerphile",
  "url": "https://www.youtube.com/watch?v=4l7Is6vOAOA",
  "words": [
    {
      "text": "So, before, we were talking about AI risk and AI safety, and",
      "start": 0.0
    },
    {
      "text": "just trying to lay out in a very generalized sort of way how general artificial intelligence can be dangerous",
      "start": 5.4
    },
    {
      "text": "and some of the types of problems it could cause,",
      "start": 11.96
    },
    {
      "text": "and just introducing the idea of AI safety or AI alignment theory",
      "start": 14.8
    },
    {
      "text": "as an area of research in computer science.",
      "start": 21.98
    },
    {
      "text": "And we also talked about superintelligence and",
      "start": 25.439
    },
    {
      "text": "the kind of problems that, the unique problems that can pose,",
      "start": 28.38
    },
    {
      "text": "and I thought what would be good is to bring it down",
      "start": 33.72
    },
    {
      "text": "to a more concrete example of",
      "start": 36.68
    },
    {
      "text": "current AI safety research that's going on now,",
      "start": 40.18
    },
    {
      "text": "and kind of give a feel for where we are,",
      "start": 44.4
    },
    {
      "text": "where humanity is on figuring these problems out.",
      "start": 48.62
    },
    {
      "text": "Supposing that we do develop a general intelligence,",
      "start": 55.02
    },
    {
      "text": "you know, an algorithm that",
      "start": 60.52
    },
    {
      "text": "actually implements general intelligence.",
      "start": 61.949
    },
    {
      "text": "How do we safely work on that thing and improve it?",
      "start": 64.29
    },
    {
      "text": "Because the situation with this stamp collector is",
      "start": 71.56
    },
    {
      "text": "from its first instant it's a superintelligence,",
      "start": 75.84
    },
    {
      "text": "so we created it with a",
      "start": 78.03
    },
    {
      "text": "certain goal, and as I said, as soon as we",
      "start": 80.22
    },
    {
      "text": "switch it on",
      "start": 82.259
    },
    {
      "text": "it's extremely dangerous. Which people",
      "start": 83.13
    },
    {
      "text": "pointed out, and it's true, you know, it was a",
      "start": 85.229
    },
    {
      "text": "thought experiment. It's true that that's",
      "start": 87.0
    },
    {
      "text": "probably not what will happen, right?",
      "start": 88.409
    },
    {
      "text": "You'll have some significantly weaker",
      "start": 89.729
    },
    {
      "text": "intelligence first that may work on",
      "start": 93.21
    },
    {
      "text": "improving itself, or we may improve it.",
      "start": 95.22
    },
    {
      "text": "So the situation where you just create",
      "start": 97.44
    },
    {
      "text": "the thing and then it goes off and does",
      "start": 99.45
    },
    {
      "text": "its own thing, either perfectly or",
      "start": 101.82
    },
    {
      "text": "terribly from the beginning, is",
      "start": 103.979
    },
    {
      "text": "unlikely. It's more likely that the thing",
      "start": 106.619
    },
    {
      "text": "will be under development.",
      "start": 108.21
    },
    {
      "text": "So then the question is, how do you make",
      "start": 109.29
    },
    {
      "text": "a system which you can teach? How do you",
      "start": 112.2
    },
    {
      "text": "create a system which",
      "start": 117.21
    },
    {
      "text": "is a general intelligence that wants",
      "start": 119.47
    },
    {
      "text": "things in the real world and is trying",
      "start": 121.33
    },
    {
      "text": "to act in the real world, but is also",
      "start": 122.77
    },
    {
      "text": "amenable to being corrected? If you",
      "start": 125.5
    },
    {
      "text": "create it with the wrong function, with",
      "start": 129.009
    },
    {
      "text": "one utility function, and you realize",
      "start": 132.07
    },
    {
      "text": "that it's doing something that actually",
      "start": 134.23
    },
    {
      "text": "you don't want it to do, how do you make it",
      "start": 135.34
    },
    {
      "text": "so that it will allow you to fix it?",
      "start": 137.56
    },
    {
      "text": "How do you make an AI which understands",
      "start": 141.25
    },
    {
      "text": "that it's unfinished, that understands",
      "start": 143.65
    },
    {
      "text": "that the utility function it's working with",
      "start": 146.68
    },
    {
      "text": "may not be the actual utility function",
      "start": 149.02
    },
    {
      "text": "it should be working with? Right, the",
      "start": 152.17
    },
    {
      "text": "utility function is what the AI cares",
      "start": 153.91
    },
    {
      "text": "about. So the stamp collecting device,",
      "start": 157.93
    },
    {
      "text": "its utility function was just \"how many",
      "start": 160.54
    },
    {
      "text": "stamps exist in the",
      "start": 162.37
    },
    {
      "text": "universe.\" This is kind of like its measure, is it?",
      "start": 164.77
    },
    {
      "text": "Yeah, it's what it is, the thing that it's",
      "start": 166.87
    },
    {
      "text": "trying to optimize in the world. The",
      "start": 169.51
    },
    {
      "text": "utility function takes in world states",
      "start": 172.66
    },
    {
      "text": "as an argument and spits out a number. It's",
      "start": 174.88
    },
    {
      "text": "basically the idea: if the world was like",
      "start": 177.7
    },
    {
      "text": "this, is that good or bad?",
      "start": 179.17
    },
    {
      "text": "And the AI is trying to steer towards",
      "start": 181.18
    },
    {
      "text": "world states that it values highly",
      "start": 184.15
    },
    {
      "text": "by that utility function.",
      "start": 186.19
    },
    {
      "text": "You don't have to explicitly build the",
      "start": 187.36
    },
    {
      "text": "AI in that way, but it will always, if",
      "start": 189.28
    },
    {
      "text": "it's behaving coherently, it will always",
      "start": 193.12
    },
    {
      "text": "behave as though it's in accordance with",
      "start": 195.31
    },
    {
      "text": "some utility function. Also before I",
      "start": 196.989
    },
    {
      "text": "talked about converging",
      "start": 199.54
    },
    {
      "text": "instrumental goals, that if you have some",
      "start": 202.42
    },
    {
      "text": "final goal, like, you know, making stamps, there",
      "start": 205.39
    },
    {
      "text": "are also instrumental goals, which are",
      "start": 208.09
    },
    {
      "text": "the goals that you do on the",
      "start": 211.09
    },
    {
      "text": "way to your final goal, right? So like",
      "start": 214.75
    },
    {
      "text": "\"acquire the capacity to do printing\" is",
      "start": 218.86
    },
    {
      "text": "perhaps an instrumental goal",
      "start": 222.67
    },
    {
      "text": "towards making stamps. But the thing is,",
      "start": 224.29
    },
    {
      "text": "there are certain goals which tend to",
      "start": 226.959
    },
    {
      "text": "pop out even across a wide variety of",
      "start": 228.79
    },
    {
      "text": "different possible terminal goals. So for",
      "start": 233.35
    },
    {
      "text": "humans, an example of",
      "start": 237.28
    },
    {
      "text": "a convergent instrumental goal would be",
      "start": 239.08
    },
    {
      "text": "money. If you want to make a lot of",
      "start": 240.76
    },
    {
      "text": "stamps, or you want to cure cancer, or you",
      "start": 246.1
    },
    {
      "text": "want to establish a moon colony, whatever",
      "start": 248.56
    },
    {
      "text": "it is, having money is a good idea, right? So",
      "start": 251.71
    },
    {
      "text": "even if you don't know what somebody",
      "start": 254.74
    },
    {
      "text": "wants, you can reasonably predict that",
      "start": 256.299
    },
    {
      "text": "they're going to value getting money,",
      "start": 258.43
    },
    {
      "text": "because money is so broadly useful. And",
      "start": 259.6
    },
    {
      "text": "before, we talked about this.",
      "start": 262.69
    },
    {
      "text": "We talked about improving your own",
      "start": 264.31
    },
    {
      "text": "intelligence as a convergent",
      "start": 265.81
    },
    {
      "text": "instrumental goal. That's another one of",
      "start": 267.46
    },
    {
      "text": "those things where it doesn't really",
      "start": 269.38
    },
    {
      "text": "matter what you're trying to achieve,",
      "start": 270.49
    },
    {
      "text": "you're probably better at achieving it if",
      "start": 271.51
    },
    {
      "text": "you're smarter. So that's something you",
      "start": 273.16
    },
    {
      "text": "can expect AIs to go for, even",
      "start": 274.99
    },
    {
      "text": "without making any assumptions about",
      "start": 278.26
    },
    {
      "text": "their final goal. So another convergent",
      "start": 279.85
    },
    {
      "text": "instrumental goal is preventing yourself",
      "start": 285.1
    },
    {
      "text": "from being destroyed. It doesn't matter",
      "start": 287.47
    },
    {
      "text": "what you want to do, you probably can't",
      "start": 290.77
    },
    {
      "text": "do it if you're destroyed. So it doesn't",
      "start": 292.33
    },
    {
      "text": "matter what the AI wants. You can have an AI",
      "start": 295.96
    },
    {
      "text": "that wants to be destroyed, in some trivial",
      "start": 298.57
    },
    {
      "text": "case. But if it does want something in",
      "start": 300.19
    },
    {
      "text": "the real world and believes that it's in",
      "start": 302.56
    },
    {
      "text": "a position to get that thing, it wants",
      "start": 303.97
    },
    {
      "text": "to be alive. Not because it wants to be",
      "start": 306.01
    },
    {
      "text": "alive",
      "start": 308.11
    },
    {
      "text": "fundamentally. It's not a survival",
      "start": 309.28
    },
    {
      "text": "instinct or an urge to live or anything",
      "start": 311.35
    },
    {
      "text": "like that. It's smoothly knowing that",
      "start": 314.08
    },
    {
      "text": "it's not going to be able to complete",
      "start": 316.15
    },
    {
      "text": "its duty, would be almost... It's going to be unable",
      "start": 318.669
    },
    {
      "text": "to achieve its goals if it's destroyed,",
      "start": 321.85
    },
    {
      "text": "and it wants to achieve that goal. So that's",
      "start": 323.53
    },
    {
      "text": "an instrumental value, preventing",
      "start": 326.26
    },
    {
      "text": "being turned off. And I'm guessing here, when we say",
      "start": 327.94
    },
    {
      "text": "\"wants,\" it's not like a machine wants.",
      "start": 330.01
    },
    {
      "text": "It's just a turn of phrase?",
      "start": 332.2
    },
    {
      "text": "Yeah, I mean, as much as anything. It's",
      "start": 334.12
    },
    {
      "text": "closer, actually. You know, I'm not",
      "start": 338.14
    },
    {
      "text": "even sure I would agree. Like if you talk",
      "start": 340.51
    },
    {
      "text": "about most machines, to talk about that",
      "start": 342.13
    },
    {
      "text": "they \"want\" to do whatever, it's not that",
      "start": 344.56
    },
    {
      "text": "meaningful because they're not agents in",
      "start": 345.97
    },
    {
      "text": "the way a general intelligence is. When a",
      "start": 348.49
    },
    {
      "text": "general intelligence wants",
      "start": 349.84
    },
    {
      "text": "something, it wants in a similar way to",
      "start": 351.85
    },
    {
      "text": "the way that people want things. So it's",
      "start": 354.34
    },
    {
      "text": "such a tight analogy that, I wouldn't",
      "start": 356.14
    },
    {
      "text": "even... I think it's totally reasonable to",
      "start": 359.56
    },
    {
      "text": "say that an AGI wants something.",
      "start": 361.72
    },
    {
      "text": "There's another slightly more subtle",
      "start": 363.47
    },
    {
      "text": "version which is closely related to not",
      "start": 365.36
    },
    {
      "text": "wanting to be turned off or destroyed,",
      "start": 367.64
    },
    {
      "text": "which is not wanting to be changed. So if",
      "start": 370.13
    },
    {
      "text": "you imagine, let's say... I mean, you have",
      "start": 376.34
    },
    {
      "text": "kids, right? Yeah.",
      "start": 380.51
    },
    {
      "text": "Suppose I were to offer you a pill or",
      "start": 382.22
    },
    {
      "text": "something. You could take this pill, and it will",
      "start": 384.83
    },
    {
      "text": "like completely rewire your brain so",
      "start": 387.32
    },
    {
      "text": "that you would just absolutely love to",
      "start": 389.87
    },
    {
      "text": "kill crickets, right? Whereas right",
      "start": 392.48
    },
    {
      "text": "now, what you want is like very",
      "start": 395.0
    },
    {
      "text": "complicated and quite difficult to",
      "start": 396.2
    },
    {
      "text": "achieve, and it's hard work for you, and",
      "start": 397.82
    },
    {
      "text": "you're probably never going to be done.",
      "start": 400.34
    },
    {
      "text": "You're never going to be truly happy, right,",
      "start": 401.45
    },
    {
      "text": "in life. Nobody is. You can't achieve",
      "start": 403.49
    },
    {
      "text": "everything you want. In this case, it",
      "start": 405.08
    },
    {
      "text": "just changes what you want. What you",
      "start": 407.21
    },
    {
      "text": "want is to kill crickets. And if you do that, you",
      "start": 408.44
    },
    {
      "text": "will be just perfectly happy and",
      "start": 410.21
    },
    {
      "text": "satisfied with life, right?",
      "start": 412.28
    },
    {
      "text": "Okay, you want to take this pill? No. Are",
      "start": 414.38
    },
    {
      "text": "you happy though?",
      "start": 416.93
    },
    {
      "text": "Yeah, I don't want to do it because... But",
      "start": 418.04
    },
    {
      "text": "that's quite a complicated specific case",
      "start": 422.51
    },
    {
      "text": "because it directly opposes what I",
      "start": 425.72
    },
    {
      "text": "currently want. It's about your",
      "start": 427.76
    },
    {
      "text": "fundamental values and goals, right? And so",
      "start": 429.86
    },
    {
      "text": "not only will you not take that pill, you",
      "start": 434.06
    },
    {
      "text": "will probably fight pretty hard to avoid",
      "start": 435.89
    },
    {
      "text": "having it administered to you.",
      "start": 438.56
    },
    {
      "text": "Yes. Because it doesn't matter how that",
      "start": 440.12
    },
    {
      "text": "future version of you would feel. You",
      "start": 443.3
    },
    {
      "text": "know that right now you love your kids,",
      "start": 445.91
    },
    {
      "text": "and you're not going to take any action",
      "start": 448.07
    },
    {
      "text": "right now which leads to them coming to",
      "start": 449.99
    },
    {
      "text": "harm.",
      "start": 452.3
    },
    {
      "text": "So it's the same thing. If you have an AI",
      "start": 453.11
    },
    {
      "text": "that, for example, values stamps, values",
      "start": 455.21
    },
    {
      "text": "collecting stamps, and you go, \"Oh, wait,",
      "start": 458.419
    },
    {
      "text": "hang on a second,",
      "start": 460.34
    },
    {
      "text": "I didn't quite do that right. Let me just",
      "start": 461.479
    },
    {
      "text": "go in and change this so that you don't",
      "start": 463.729
    },
    {
      "text": "like stamps quite so much,\" it's going to",
      "start": 465.2
    },
    {
      "text": "say, \"But the only important thing is",
      "start": 467.6
    },
    {
      "text": "stamps! If you change me, I'm not going",
      "start": 469.79
    },
    {
      "text": "to collect as many stamps, which is",
      "start": 472.28
    },
    {
      "text": "something I don't want.\" There's a general",
      "start": 473.57
    },
    {
      "text": "tendency for AGI to try and prevent you",
      "start": 475.16
    },
    {
      "text": "from modifying it once it's running.",
      "start": 480.11
    },
    {
      "text": "I can understand that now, in the context",
      "start": 482.63
    },
    {
      "text": "we're talking about, right?",
      "start": 485.27
    },
    {
      "text": "Because that's it. In almost any",
      "start": 487.37
    },
    {
      "text": "situation, being given a new utility",
      "start": 490.94
    },
    {
      "text": "function is going to rate very low on",
      "start": 493.07
    },
    {
      "text": "your current utility function.",
      "start": 495.71
    },
    {
      "text": "Okay, so that's a problem.",
      "start": 497.66
    },
    {
      "text": "How do you... if you want to build",
      "start": 501.199
    },
    {
      "text": "something that you can teach, that means",
      "start": 503.27
    },
    {
      "text": "you want to be able to change its",
      "start": 504.949
    },
    {
      "text": "utility function, and you don't want it to",
      "start": 506.75
    },
    {
      "text": "fight you",
      "start": 507.979
    },
    {
      "text": "on it, right? 100%, yeah.",
      "start": 514.12
    },
    {
      "text": "So this has been formalized as this",
      "start": 522.849
    },
    {
      "text": "property that we want early AGI to have,",
      "start": 526.39
    },
    {
      "text": "called corrigibility. That is to say, it's",
      "start": 529.33
    },
    {
      "text": "open to being corrected.",
      "start": 531.73
    }
  ]
}