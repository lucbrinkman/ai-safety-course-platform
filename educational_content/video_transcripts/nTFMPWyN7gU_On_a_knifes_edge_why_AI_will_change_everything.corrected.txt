Hi everyone. Technology has advanced rapidly in recent decades, yet daily life remains pretty much unchanged. But this sense of stability is an illusion. We're now balanced on a knife's edge because AI isn't just another invention. It's fundamentally different. AI will soon reshape our lives and our societies in ways we've never imagined. Keep watching to learn more. This video has three parts: From H2O to Singularity, AI is a Faster Feedback Loop, and A Momentous Moment.

Part One: From H2O to Singularity

The history of human development is exponential. I'm referring to the development of societal and technological progress. Of course, what does this really mean? It means that there is no normal. There is only "this is how the world is today." And the only thing you know for sure is that that state is actually constantly changing. New innovations are created and people's lives look a little different than they did in the past.

And while you can try to predict the future based on what was happening in the past and especially based on your own lived experience, this has its limitations. In particular, it causes you to underestimate technological progress because the rate of change is exponential and things are actually getting faster, faster, so to speak. The development that happened in the past 10 years is not what the next 10 years will look like. It might be what the next six years looks like or whatever depending on the rate of change. Humans seem to think pretty linearly. So any type of nonlinear function, especially an exponential function, is very difficult for people to predict. If you want more examples about this, you can check out this AGI timelines video that I made previously where I go into technological S-curves and whatnot.

Why does technology improve exponentially? Why does the rate of change constantly get faster and faster? Roughly speaking, it's because at any given stage, you can leverage all that you've already learned.

A key aspect of innovation as well is the speed of feedback cycles. For example, suppose you're learning to play chess and you've never really played chess before. So, you have to learn from the very beginning. If you tried to play chess by mail, in other words, you write down the move, you put that in a letter, and you snail mail it to somebody else, and then they write down their move, and they snail mail it back to you, etc. That would mean that every move would take several days or weeks. And at that rate, as a beginner to chess, you wouldn't really be learning the game very fast at all. You'd probably forget the entire context of the game by the time it was your turn to make another move.

In contrast, if you're sitting across from another person and you're playing the game in person, then you get to make a move every few minutes instead of every few days or weeks. The speed of the feedback mechanism there is going to mean that you learn the game much more rapidly. Now imagine that instead of playing against a human who also takes a bunch of time to think about their moves, you were actually instead playing against a chess AI. This AI might move pretty much instantly. So you wouldn't have to wait for your opponent to move. You would just be spending all your time thinking about chess and making moves and getting immediate feedback about how well those actions were working. And of course, you could engage in games at any time and not just when you happen to be sitting down with another person and a chessboard.

If you're trying to learn something, if you're trying to invent something, then getting the feedback cycles as short as possible will really maximize your chance of success. The reason we're talking about feedback cycles is because the same thing has happened throughout history with evolution and human development and so on. You can look at it all as a series of feedback cycles that are getting faster and faster.

For example, physics and chemistry have a fair amount of complexity built into them. And you can even get chemical reactions that are self-sustaining and have other interesting properties. But it took 600 million years after the formation of planet Earth before chemical reactions actually came together and RNA, DNA, and hence life was actually created. We don't know very much about how that happened at this point, but we can guess that it had very high variance. In other words, it could have taken 10 times longer. And this is because the speed of the feedback cycles, if you want to call it, on the experiments that chemistry was performing on primordial Earth is quite slow.

Once you switch from chemistry into biology, then evolution starts happening at the scale of individuals. Right? Each creature that is born has a particular set of DNA which could get randomly mutated and combined with other DNA if that creature has offspring. Now, this is a much faster feedback cycle than with chemistry because you have this mechanism that's intentionally self-propagating and carrying information forward about what works best in the environment. But you still have to wait for many generations before meaningful change takes place. And if you fast forward to mammals, well, each generation is going to be at least years or even decades before you get a new set of individuals. And even though brains started increasing in complexity a lot, especially after the death of the dinosaurs, there's still very limited information passed along in this fashion. It's just your genes. It doesn't really matter what else that a creature managed to do in their lifetime.

That changed with the invention of language, with what I like to call societal evolution. When you have language, information can be passed on much more quickly, not only to one's descendants, but also to everyone else who's alive at the same time, everyone in the society. That's why societies can evolve many structures and have different sorts of knowledge like how to make certain tools or whatever that don't depend on the genetics of the people involved.

Nevertheless, societies tend to evolve relatively slowly by modern standards. For example, think about how much technological change happened between the year AD 0 and the year AD 1000. You know, by modern standards, everybody's life would have looked pretty much the same at the beginning and at the end. But when you get to technological evolution, a single person can experience several major transformations in their lifetime. Think like the invention of the automobile or mobile phones or the internet, etc. And this is when we really first start to feel as individuals the impact of the human development curve, right? We can look back and say, you know, when I was young, computers didn't exist or whatever. No, I'm not that old.

So, that's essentially where we stand. But we're actually entering another phase of evolution which is what I'm going to call intellectual evolution. And we're starting to see important innovations happening like once a year or something like that which is much faster than the previous rate of innovation. Once AI really starts to take off and automated intelligence is the main force driving technological innovation rather than human brains, important changes could be happening really, really rapidly indeed. For example, perhaps every day a new transformation would appear, perhaps even every second. This is called the Singularity because we wouldn't be the driving force there. And so it's really hard for us to predict what would actually happen and the rate of change that would actually be present.

But the point is that evolution has been taking place in a series of faster and faster feedback loops because evolution needs feedback from the environment to decide what is the most fit individual, the best idea, the best technology, whatever. And you'll probably have noticed that in the fastest feedback loops that I'm talking about here, humans aren't really playing a really big role. It's actually AI that's taking the helm. So, what does that really mean? Let's dive into it more in the next section.

Part Two: AI is a Faster Feedback Loop

AI is not like other technologies. It doesn't just make it faster or cheaper to do everything that we were already doing before. It establishes a faster feedback loop of reasoning that doesn't involve any humans in the loop. There's nothing to prevent AI from becoming faster and faster and solving problems that were truly infeasible before. And importantly, AI can even improve AI technology eventually in the long run, meaning that as a society, we will slowly reduce our reliance on human brains for solving our most difficult problems. And eventually, humans will probably be out of the loop in pretty much every intellectual area, the same way that human traders are currently out of the loop entirely in high-frequency trading.

Sure, for a while you just go to a higher level of abstraction and write code to do transactions instead of doing the transactions yourself. And that's kind of where the financial industry is right now. But AI isn't just good at low-level things. It doesn't just force you up to a higher level of abstraction, which is what most technologies do. AI is good at literally every intellectual endeavor, or it will be in the long run.

So, let's talk about human jobs for a minute. Like I said, generally as automation or economic advancement take hold, people start doing more and more abstract tasks which are also higher paying, but they tend to rely heavily on intellectual capacity. Furthermore, the new more abstract tasks are basically higher up on a pyramid and therefore they often don't need as many people doing them. So people lose their jobs when a factory gets fully automated, for example. But the economy is diverse and new types of jobs are popping up, especially those related to the automation itself. So, as a species, we kind of bumble through.

But that's not going to happen this time. At some point, the rate of jobs becoming obsolete will exceed the rate of new jobs being created, which has arguably already happened in some creative fields like digital art. This will happen simply because AIs are cheaper and sometimes better at doing the tasks than humans are. AIs don't need to sleep or eat. They don't get tired or take time off. And an upgrade can be applied immediately to all the AIs in the workforce at the same time instead of having to individually train each new human on how the new technology works, which is very important when you have major changes happening at a very fast rate.

Here's a bit of a story or an analogy. When humans domesticated work animals like horses and oxen thousands of years ago, we got to take a step back and let them do the heavy work of transportation and milling grain and whatever. When humans invented the steam engine, we got to take a step back from even more types of physical work. And we gained new abilities like the ability to forge steel in large quantities and cross the ocean in days in steamboats. We similarly gained new capabilities when we invented the computer. Global communication, space flight, and computer games all became possible.

As we develop AI, we get to step back from doing heavy intellectual work. For example, I wrote a program in a very unfamiliar domain to me the other day and my AI coding assistant wrote two pieces, two very important pieces of the code for me. It would have taken a lot more time and effort before, but like I said, we've been able to take a step back on this really heavy intellectual work. But as AI gets better and better and we're stepping back further and further, we basically bump into a wall. We can't step back anymore. There's nothing left for us to do.

In fact, we risk abdicating our position as the intelligent species on this planet that is charting our own destiny. Because what does it mean to chart? It's just the accumulation of a lot of different decisions made in society. Decisions made by whatever intelligence was being brought to bear. And when that intelligence is 99% AI or 100% AI, what is left of being human? What can we still do and enjoy?

Part Three: A Momentous Moment

As someone told me recently, this is a momentous moment, which sounds a bit redundant, but I don't think it is. It's a momentous moment in the history of life on Earth because it's the beginning of artificial life, the beginning of a vast acceleration in feedback cycles that evolution kicked off so long ago. If you look at this graph from Wait But Why, it describes the situation very well. We look back and we see this exponential curve and we look ahead and we see a curve that shoots upwards in terms of the amount of technological progress. That's what we seem to have in our future.

And people always want to know, of course, is this amazing or is this terrible? The answer is that you have to sit with both perspectives. You have to internalize a bit of 1984 and do some doublethink because the set of possibilities ahead of us includes a lot of positive and negative scenarios.

On the one hand, this acceleration of progress could make things possible that are just unthinkable today. I'm talking about things that we would consider science fiction like space travel and nanomedicine and stellar engineering and Dyson spheres. In fact, the article I took that graph from by Wait But Why actually says that once you reach superintelligence, the extreme outcomes become the most likely. The really positive and the really negative both become more likely. And for humans, for us humans, that means either immortality or extinction, which is a pretty heavy way of looking at it.

But it's true because the positives are so immense that you could imagine inventing the medicine to keep us alive forever or uploading ourselves to digital minds or whatever the situation might be. But on the negative side, there are a lot of dangers inherent in advanced AI development. For example, there are risks of misuse by humans who are seeking power. There are risks of AI systems misunderstanding the goals that we give them. There are even risks of humans losing control of their AI systems who might then pursue their own goals or own survival like a new species would.

Here's a quote: "Mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war." That quote is from an open letter signed by many of the top scientists, business people, and engineers in the world.

I actually work in AI safety myself. The goal isn't necessarily to scare everybody, but rather to understand and recognize the risks that could happen and then try to make sure that they never do happen. But it's really hard for everyone involved in making important decisions to realize the risks even once researchers have figured it out and then to coordinate on acting appropriately.

Strangely enough, this is one way in which we are inferior to digital intelligence. Like I said, digital intelligence or AIs can just send an upgrade to everybody and now all of a sudden you can coordinate much more easily. For example, we need coordination in politics. AI safety is not just a technical problem. It's a sociotechnical problem. It requires coordination amongst governments to all do the safe thing because if one group goes rogue, it endangers everybody.

Unfortunately, political administrations around the world, especially the US and France, seem to be treating AI as just another technology to get ahead in. Again, it's not. This is a transformative technology that represents a step change in the speed of evolution. It can and will overwhelm us as a species if we don't pay attention. At the same time, the upside could be so immense as to trivialize many of our greatest achievements to date. So, we can and should continue pursuing this technology. We just have to keep our heads looking in the right direction.

Let's stop thinking like primates in the jungle and jumping to conclusions and just looking out for ourselves because the stakes are about as high as they can possibly go. We've sort of gotten used to the risk of somebody pressing a big red button and starting a nuclear war. But this too is different. At the risk of anthropomorphizing, this is more like we have produced a child, a genius that we know is not bound by our own biological limits on intelligence and who is learning at a frightening pace. A child who will most likely be taking care of us one day as we get older. So what should we be teaching it?

Finally, in conclusion, evolution has consisted of a series of faster and faster feedback cycles as we go from chemical to biological to societal, technological, and so on. And AI represents a new and faster phase of that evolution. As humans delegate more and more intellectual tasks to AIs, we will eventually become out of the loop on most intellectually important things. It may be a very gradual process. It's not like boom, suddenly the AIs are in control. Although that too is possible if we have a loss of control scenario, but we have to think to ourselves, what do we actually want as we take this giant step back from heavy intellectual labor? Unfortunately, the world doesn't stay the same. The world and our technology is not static, even though it might look like it locally around you.

Let me know in the comments how you liked this kind of higher level video from me. It's an experiment I'm trying. And I would really like to thank the person who told me "this is a momentous moment." You know who you are. Make sure to follow me and subscribe. And you can check out Twitter/X and other platforms where I am present. And if you liked this video, check out this previous one I made about what to do in a world where AGI is coming soon. All right, that's all I have for today. Thank you very much for watching. Bye.
